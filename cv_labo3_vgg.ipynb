{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RodrigoGuedesDP/Computer_Vision/blob/main/cv_labo3_vgg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Computer Vision - Clasificación de expresiones faciales con VGG16\n",
        "**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "*   En este laboratorio se construira usara el modelo VGG16 pre-entrenado para construir un clasificador con estos datos y evaluar su rendimiento. Para ello se utilizará un dataset público llamado \"FER-2013\".\n",
        "\n",
        "**Autores:**  \n",
        "\n",
        "Nieto Espinoza, Brajan E.  \n",
        "[brajan.nieto@utec.edu.pe](mailto:brajan.nieto@utec.edu.pe)\n",
        "\n",
        "Guedes del Pozo,  Rodrigo J.  \n",
        "[rodrigo.guedes.d@utec.edu.pe](mailto:rodrigo.guedes.d@utec.edu.pe)\n",
        "\n",
        "<img src=\"https://pregrado.utec.edu.pe/sites/default/files/logo-utec-h_0_0.svg\" width=\"190\" alt=\"Logo UTEC\" loading=\"lazy\" typeof=\"foaf:Image\">      \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "oCMpUZugiIYM"
      },
      "id": "oCMpUZugiIYM"
    },
    {
      "cell_type": "markdown",
      "id": "8b98d600",
      "metadata": {
        "id": "8b98d600"
      },
      "source": [
        "# Sesión 08: Arquitectura VGG\n",
        "\n",
        "En esta sesión estudiaremos la **arquitectura VGG**, una de las redes neuronales convolucionales más influyentes en visión por computador. La red VGG, propuesta por el *Visual Geometry Group* de la Universidad de Oxford, se caracteriza por utilizar filtros de **3×3** de forma uniforme y apilar muchas capas para capturar patrones de las imágenes.\n",
        "\n",
        "Exploraremos cómo cargar el modelo VGG16 utilizando `torchvision`, analizaremos su número de parámetros y construiremos la arquitectura **desde cero** con PyTorch para comprender su estructura interna. También aprenderemos a usar un modelo VGG16 preentrenado en ImageNet para clasificar imágenes nuevas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c802f752",
      "metadata": {
        "id": "c802f752"
      },
      "source": [
        "## Cargando la arquitectura VGG desde `torchvision`\n",
        "\n",
        "Para comenzar, cargaremos la clase `VGG16` desde `torchvision.models`. Al especificar `weights=None` obtendremos la arquitectura sin pesos preentrenados. A continuación mostramos un resumen del modelo (capas, tamaños de salida y número de parámetros de cada capa) utilizando la utilidad `torchinfo.summary`.\n",
        "\n",
        "> **Nota:** Para usar `summary` se necesita instalar el paquete `torchinfo` (si no está instalado en tu entorno). Google Colab ya incluye PyTorch y Torchvision, pero puedes instalar dependencias adicionales mediante `pip`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e97d49d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e97d49d",
        "outputId": "1c6a0925-08a4-4296-a2d8-91351dbe5844"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "VGG                                      [1, 1000]                 --\n",
              "├─Sequential: 1-1                        [1, 512, 7, 7]            --\n",
              "│    └─Conv2d: 2-1                       [1, 64, 224, 224]         1,792\n",
              "│    └─ReLU: 2-2                         [1, 64, 224, 224]         --\n",
              "│    └─Conv2d: 2-3                       [1, 64, 224, 224]         36,928\n",
              "│    └─ReLU: 2-4                         [1, 64, 224, 224]         --\n",
              "│    └─MaxPool2d: 2-5                    [1, 64, 112, 112]         --\n",
              "│    └─Conv2d: 2-6                       [1, 128, 112, 112]        73,856\n",
              "│    └─ReLU: 2-7                         [1, 128, 112, 112]        --\n",
              "│    └─Conv2d: 2-8                       [1, 128, 112, 112]        147,584\n",
              "│    └─ReLU: 2-9                         [1, 128, 112, 112]        --\n",
              "│    └─MaxPool2d: 2-10                   [1, 128, 56, 56]          --\n",
              "│    └─Conv2d: 2-11                      [1, 256, 56, 56]          295,168\n",
              "│    └─ReLU: 2-12                        [1, 256, 56, 56]          --\n",
              "│    └─Conv2d: 2-13                      [1, 256, 56, 56]          590,080\n",
              "│    └─ReLU: 2-14                        [1, 256, 56, 56]          --\n",
              "│    └─Conv2d: 2-15                      [1, 256, 56, 56]          590,080\n",
              "│    └─ReLU: 2-16                        [1, 256, 56, 56]          --\n",
              "│    └─MaxPool2d: 2-17                   [1, 256, 28, 28]          --\n",
              "│    └─Conv2d: 2-18                      [1, 512, 28, 28]          1,180,160\n",
              "│    └─ReLU: 2-19                        [1, 512, 28, 28]          --\n",
              "│    └─Conv2d: 2-20                      [1, 512, 28, 28]          2,359,808\n",
              "│    └─ReLU: 2-21                        [1, 512, 28, 28]          --\n",
              "│    └─Conv2d: 2-22                      [1, 512, 28, 28]          2,359,808\n",
              "│    └─ReLU: 2-23                        [1, 512, 28, 28]          --\n",
              "│    └─MaxPool2d: 2-24                   [1, 512, 14, 14]          --\n",
              "│    └─Conv2d: 2-25                      [1, 512, 14, 14]          2,359,808\n",
              "│    └─ReLU: 2-26                        [1, 512, 14, 14]          --\n",
              "│    └─Conv2d: 2-27                      [1, 512, 14, 14]          2,359,808\n",
              "│    └─ReLU: 2-28                        [1, 512, 14, 14]          --\n",
              "│    └─Conv2d: 2-29                      [1, 512, 14, 14]          2,359,808\n",
              "│    └─ReLU: 2-30                        [1, 512, 14, 14]          --\n",
              "│    └─MaxPool2d: 2-31                   [1, 512, 7, 7]            --\n",
              "├─AdaptiveAvgPool2d: 1-2                 [1, 512, 7, 7]            --\n",
              "├─Sequential: 1-3                        [1, 1000]                 --\n",
              "│    └─Linear: 2-32                      [1, 4096]                 102,764,544\n",
              "│    └─ReLU: 2-33                        [1, 4096]                 --\n",
              "│    └─Dropout: 2-34                     [1, 4096]                 --\n",
              "│    └─Linear: 2-35                      [1, 4096]                 16,781,312\n",
              "│    └─ReLU: 2-36                        [1, 4096]                 --\n",
              "│    └─Dropout: 2-37                     [1, 4096]                 --\n",
              "│    └─Linear: 2-38                      [1, 1000]                 4,097,000\n",
              "==========================================================================================\n",
              "Total params: 138,357,544\n",
              "Trainable params: 138,357,544\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 15.48\n",
              "==========================================================================================\n",
              "Input size (MB): 0.60\n",
              "Forward/backward pass size (MB): 108.45\n",
              "Params size (MB): 553.43\n",
              "Estimated Total Size (MB): 662.49\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Instalamos la biblioteca torchinfo en caso de que no esté disponible\n",
        "!pip install -q torchinfo\n",
        "\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "from torchinfo import summary\n",
        "\n",
        "# Cargar la arquitectura VGG16 sin pesos preentrenados\n",
        "vgg16_arch = models.vgg16(weights=None)\n",
        "\n",
        "# Mostramos un resumen del modelo. La entrada es una imagen RGB de 224×224\n",
        "summary(vgg16_arch, input_size=(1, 3, 224, 224))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f52f8d69",
      "metadata": {
        "id": "f52f8d69"
      },
      "source": [
        "## Número total de parámetros de VGG16\n",
        "\n",
        "El número de parámetros de una red es una medida de su complejidad y capacidad de representación. Podemos sumar el número de elementos de cada tensor de parámetros utilizando `p.numel()` para obtener el total de parámetros entrenables.  \n",
        "\n",
        "La documentación oficial de PyTorch indica que la variante VGG16 preentrenada contiene **138 357 544** parámetros. Comprobemos este valor mediante código:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f9c094b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f9c094b",
        "outputId": "956cfc46-b7fd-4d7b-8619-391541811817"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de parámetros (sin pesos preentrenados): 138357544 ≈ 138.36 millones\n"
          ]
        }
      ],
      "source": [
        "# Calcular el número total de parámetros de la arquitectura VGG16\n",
        "# Sumamos todos los elementos de cada tensor de parámetros\n",
        "total_params = sum(p.numel() for p in vgg16_arch.parameters())\n",
        "print(f\"Total de parámetros (sin pesos preentrenados): {total_params} ≈ {total_params/1e6:.2f} millones\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3acaf17d",
      "metadata": {
        "id": "3acaf17d"
      },
      "source": [
        "## VGG16 desde cero con PyTorch\n",
        "\n",
        "Para comprender mejor la estructura de VGG16 implementaremos la red manualmente. VGG16 sigue un patrón simple: bloques de convoluciones 3×3 repetidas, seguidos de max pooling 2×2 para reducir la resolución. La configuración típica de VGG16 es la siguiente:\n",
        "\n",
        "- **Bloque 1:** Conv 64 → Conv 64 → MaxPool 2×2  \n",
        "- **Bloque 2:** Conv 128 → Conv 128 → MaxPool 2×2  \n",
        "- **Bloque 3:** Conv 256 → Conv 256 → Conv 256 → MaxPool 2×2  \n",
        "- **Bloque 4:** Conv 512 → Conv 512 → Conv 512 → MaxPool 2×2  \n",
        "- **Bloque 5:** Conv 512 → Conv 512 → Conv 512 → MaxPool 2×2  \n",
        "\n",
        "Después de los bloques convolucionales, la salida se aplana y pasa a través de tres capas completamente conectadas (`Linear`) con 4096, 4096 y `num_classes` neuronas, respectivamente. Esta estructura, junto con el uso constante de filtros 3×3, permite captar patrones detallados sin aumentar excesivamente el número de parámetros.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57807e76",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57807e76",
        "outputId": "22b28db9-a34e-46f0-d0ef-449c51e22781"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "VGG                                      [1, 1000]                 --\n",
              "├─Conv2d: 1-1                            [1, 64, 224, 224]         1,792\n",
              "├─ReLU: 1-2                              [1, 64, 224, 224]         --\n",
              "├─Conv2d: 1-3                            [1, 64, 224, 224]         36,928\n",
              "├─ReLU: 1-4                              [1, 64, 224, 224]         --\n",
              "├─MaxPool2d: 1-5                         [1, 64, 112, 112]         --\n",
              "├─Conv2d: 1-6                            [1, 128, 112, 112]        73,856\n",
              "├─ReLU: 1-7                              [1, 128, 112, 112]        --\n",
              "├─Conv2d: 1-8                            [1, 128, 112, 112]        147,584\n",
              "├─ReLU: 1-9                              [1, 128, 112, 112]        --\n",
              "├─MaxPool2d: 1-10                        [1, 128, 56, 56]          --\n",
              "├─Conv2d: 1-11                           [1, 256, 56, 56]          295,168\n",
              "├─ReLU: 1-12                             [1, 256, 56, 56]          --\n",
              "├─Conv2d: 1-13                           [1, 256, 56, 56]          590,080\n",
              "├─ReLU: 1-14                             [1, 256, 56, 56]          --\n",
              "├─Conv2d: 1-15                           [1, 256, 56, 56]          590,080\n",
              "├─ReLU: 1-16                             [1, 256, 56, 56]          --\n",
              "├─MaxPool2d: 1-17                        [1, 256, 28, 28]          --\n",
              "├─Conv2d: 1-18                           [1, 512, 28, 28]          1,180,160\n",
              "├─ReLU: 1-19                             [1, 512, 28, 28]          --\n",
              "├─Conv2d: 1-20                           [1, 512, 28, 28]          2,359,808\n",
              "├─ReLU: 1-21                             [1, 512, 28, 28]          --\n",
              "├─Conv2d: 1-22                           [1, 512, 28, 28]          2,359,808\n",
              "├─ReLU: 1-23                             [1, 512, 28, 28]          --\n",
              "├─MaxPool2d: 1-24                        [1, 512, 14, 14]          --\n",
              "├─Conv2d: 1-25                           [1, 512, 14, 14]          2,359,808\n",
              "├─ReLU: 1-26                             [1, 512, 14, 14]          --\n",
              "├─Conv2d: 1-27                           [1, 512, 14, 14]          2,359,808\n",
              "├─ReLU: 1-28                             [1, 512, 14, 14]          --\n",
              "├─Conv2d: 1-29                           [1, 512, 14, 14]          2,359,808\n",
              "├─ReLU: 1-30                             [1, 512, 14, 14]          --\n",
              "├─MaxPool2d: 1-31                        [1, 512, 7, 7]            --\n",
              "├─AdaptiveAvgPool2d: 1-32                [1, 512, 7, 7]            --\n",
              "├─Linear: 1-33                           [1, 4096]                 102,764,544\n",
              "├─ReLU: 1-34                             [1, 4096]                 --\n",
              "├─Dropout: 1-35                          [1, 4096]                 --\n",
              "├─Linear: 1-36                           [1, 4096]                 16,781,312\n",
              "├─ReLU: 1-37                             [1, 4096]                 --\n",
              "├─Dropout: 1-38                          [1, 4096]                 --\n",
              "├─Linear: 1-39                           [1, 1000]                 4,097,000\n",
              "==========================================================================================\n",
              "Total params: 138,357,544\n",
              "Trainable params: 138,357,544\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 15.48\n",
              "==========================================================================================\n",
              "Input size (MB): 0.60\n",
              "Forward/backward pass size (MB): 108.45\n",
              "Params size (MB): 553.43\n",
              "Estimated Total Size (MB): 662.49\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchinfo import summary\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementación de la arquitectura VGG16.\n",
        "    Args:\n",
        "        num_classes (int): número de clases de salida (por defecto 1000).\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes: int = 1000) -> None:\n",
        "        super().__init__()\n",
        "        # Funciones de activación, pooling y dropout reutilizables\n",
        "        self.relu    = nn.ReLU(inplace=True)\n",
        "        self.pool    = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout = nn.Dropout()\n",
        "\n",
        "        # Bloque 1\n",
        "        self.conv1_1 = nn.Conv2d(3,   64, kernel_size=3, padding=1)\n",
        "        self.conv1_2 = nn.Conv2d(64,  64, kernel_size=3, padding=1)\n",
        "\n",
        "        # Bloque 2\n",
        "        self.conv2_1 = nn.Conv2d(64,  128, kernel_size=3, padding=1)\n",
        "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "\n",
        "        # Bloque 3\n",
        "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "\n",
        "        # Bloque 4\n",
        "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "\n",
        "        # Bloque 5\n",
        "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "\n",
        "        # Agrupación y clasificador\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
        "        self.fc1 = nn.Linear(512 * 7 * 7, 4096)\n",
        "        self.fc2 = nn.Linear(4096, 4096)\n",
        "        self.fc3 = nn.Linear(4096, num_classes)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # Bloque 1\n",
        "        x = self.relu(self.conv1_1(x))\n",
        "        x = self.relu(self.conv1_2(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Bloque 2\n",
        "        x = self.relu(self.conv2_1(x))\n",
        "        x = self.relu(self.conv2_2(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Bloque 3\n",
        "        x = self.relu(self.conv3_1(x))\n",
        "        x = self.relu(self.conv3_2(x))\n",
        "        x = self.relu(self.conv3_3(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Bloque 4\n",
        "        x = self.relu(self.conv4_1(x))\n",
        "        x = self.relu(self.conv4_2(x))\n",
        "        x = self.relu(self.conv4_3(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Bloque 5\n",
        "        x = self.relu(self.conv5_1(x))\n",
        "        x = self.relu(self.conv5_2(x))\n",
        "        x = self.relu(self.conv5_3(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Agrupación y capas densas\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout(self.relu(self.fc1(x)))\n",
        "        x = self.dropout(self.relu(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Creamos una instancia del modelo VGG16\n",
        "vgg16_custom = VGG(num_classes=1000)\n",
        "\n",
        "# Resumen de la red personalizada para verificar coincidencia con VGG16\n",
        "summary(vgg16_custom, input_size=(1, 3, 224, 224))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c02e818",
      "metadata": {
        "id": "7c02e818"
      },
      "source": [
        "En el resumen anterior, la arquitectura de VGG16 implementada manualmente coincide con la versión proporcionada por `torchvision`. Observa que el número total de parámetros también se aproxima a 138 millones, igual que la versión preentrenada.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f8b75d6",
      "metadata": {
        "id": "6f8b75d6"
      },
      "source": [
        "## Cargando VGG16 preentrenado en ImageNet\n",
        "\n",
        "`torchvision` permite cargar modelos VGG16 con pesos preentrenados en el conjunto de datos **ImageNet**. Estos modelos han aprendido representaciones generales de objetos y pueden utilizarse para clasificación o como extractores de características en otras tareas.  \n",
        "\n",
        "Al cargar el modelo con `weights='IMAGENET1K_V1'` obtendremos automáticamente los pesos entrenados. A continuación mostraremos un resumen de la arquitectura preentrenada.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9c09f8b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9c09f8b",
        "outputId": "4619a64c-9d39-4a89-fdd5-3c49b8708733"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 528M/528M [00:07<00:00, 77.2MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "VGG                                      [1, 1000]                 --\n",
              "├─Sequential: 1-1                        [1, 512, 7, 7]            --\n",
              "│    └─Conv2d: 2-1                       [1, 64, 224, 224]         1,792\n",
              "│    └─ReLU: 2-2                         [1, 64, 224, 224]         --\n",
              "│    └─Conv2d: 2-3                       [1, 64, 224, 224]         36,928\n",
              "│    └─ReLU: 2-4                         [1, 64, 224, 224]         --\n",
              "│    └─MaxPool2d: 2-5                    [1, 64, 112, 112]         --\n",
              "│    └─Conv2d: 2-6                       [1, 128, 112, 112]        73,856\n",
              "│    └─ReLU: 2-7                         [1, 128, 112, 112]        --\n",
              "│    └─Conv2d: 2-8                       [1, 128, 112, 112]        147,584\n",
              "│    └─ReLU: 2-9                         [1, 128, 112, 112]        --\n",
              "│    └─MaxPool2d: 2-10                   [1, 128, 56, 56]          --\n",
              "│    └─Conv2d: 2-11                      [1, 256, 56, 56]          295,168\n",
              "│    └─ReLU: 2-12                        [1, 256, 56, 56]          --\n",
              "│    └─Conv2d: 2-13                      [1, 256, 56, 56]          590,080\n",
              "│    └─ReLU: 2-14                        [1, 256, 56, 56]          --\n",
              "│    └─Conv2d: 2-15                      [1, 256, 56, 56]          590,080\n",
              "│    └─ReLU: 2-16                        [1, 256, 56, 56]          --\n",
              "│    └─MaxPool2d: 2-17                   [1, 256, 28, 28]          --\n",
              "│    └─Conv2d: 2-18                      [1, 512, 28, 28]          1,180,160\n",
              "│    └─ReLU: 2-19                        [1, 512, 28, 28]          --\n",
              "│    └─Conv2d: 2-20                      [1, 512, 28, 28]          2,359,808\n",
              "│    └─ReLU: 2-21                        [1, 512, 28, 28]          --\n",
              "│    └─Conv2d: 2-22                      [1, 512, 28, 28]          2,359,808\n",
              "│    └─ReLU: 2-23                        [1, 512, 28, 28]          --\n",
              "│    └─MaxPool2d: 2-24                   [1, 512, 14, 14]          --\n",
              "│    └─Conv2d: 2-25                      [1, 512, 14, 14]          2,359,808\n",
              "│    └─ReLU: 2-26                        [1, 512, 14, 14]          --\n",
              "│    └─Conv2d: 2-27                      [1, 512, 14, 14]          2,359,808\n",
              "│    └─ReLU: 2-28                        [1, 512, 14, 14]          --\n",
              "│    └─Conv2d: 2-29                      [1, 512, 14, 14]          2,359,808\n",
              "│    └─ReLU: 2-30                        [1, 512, 14, 14]          --\n",
              "│    └─MaxPool2d: 2-31                   [1, 512, 7, 7]            --\n",
              "├─AdaptiveAvgPool2d: 1-2                 [1, 512, 7, 7]            --\n",
              "├─Sequential: 1-3                        [1, 1000]                 --\n",
              "│    └─Linear: 2-32                      [1, 4096]                 102,764,544\n",
              "│    └─ReLU: 2-33                        [1, 4096]                 --\n",
              "│    └─Dropout: 2-34                     [1, 4096]                 --\n",
              "│    └─Linear: 2-35                      [1, 4096]                 16,781,312\n",
              "│    └─ReLU: 2-36                        [1, 4096]                 --\n",
              "│    └─Dropout: 2-37                     [1, 4096]                 --\n",
              "│    └─Linear: 2-38                      [1, 1000]                 4,097,000\n",
              "==========================================================================================\n",
              "Total params: 138,357,544\n",
              "Trainable params: 138,357,544\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 15.48\n",
              "==========================================================================================\n",
              "Input size (MB): 0.60\n",
              "Forward/backward pass size (MB): 108.45\n",
              "Params size (MB): 553.43\n",
              "Estimated Total Size (MB): 662.49\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import torchvision.models as models\n",
        "from torchinfo import summary\n",
        "\n",
        "# Cargar VGG16 con pesos preentrenados en ImageNet\n",
        "vgg16_pretrained = models.vgg16(weights='IMAGENET1K_V1')\n",
        "\n",
        "# Mostrar resumen del modelo preentrenado\n",
        "summary(vgg16_pretrained, input_size=(1, 3, 224, 224))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57345cb6",
      "metadata": {
        "id": "57345cb6"
      },
      "source": [
        "## Uso del VGG16 preentrenado para clasificar imágenes\n",
        "\n",
        "Una de las principales aplicaciones de VGG16 es clasificar imágenes en las 1000 categorías de ImageNet. Para realizar inferencia:\n",
        "\n",
        "1. **Preprocesamiento:** La imagen debe redimensionarse a 256 × 256, realizar un recorte central de 224 × 224 y normalizarse con las medias y desviaciones estándar recomendadas. Estos pasos pueden obtenerse directamente de `VGG16_Weights.IMAGENET1K_V1.transforms()`.\n",
        "2. **Inferencia:** Se pasa la imagen a través de la red en modo evaluación (`model.eval()`), se obtiene la salida y se aplica `argmax` para seleccionar la clase de mayor probabilidad.\n",
        "3. **Decodificación:** La lista de categorías está disponible en `weights.meta['categories']` y permite traducir el índice de la clase al nombre legible.\n",
        "\n",
        "A continuación se muestra una función que encapsula todo el proceso. La función recibe la ruta de la imagen, realiza el preprocesamiento, ejecuta la inferencia y devuelve la clase predicha.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44afc53c",
      "metadata": {
        "id": "44afc53c"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import vgg16, VGG16_Weights\n",
        "\n",
        "# Función para cargar el modelo preentrenado y las transformaciones\n",
        "def load_pretrained_vgg():\n",
        "    weights = VGG16_Weights.IMAGENET1K_V1\n",
        "    model = vgg16(weights=weights)\n",
        "    model.eval()  # establecer en modo evaluación\n",
        "    preprocess = weights.transforms()  # incluye resize, center crop y normalización\n",
        "    categories = weights.meta['categories']\n",
        "    return model, preprocess, categories\n",
        "\n",
        "# Función para clasificar una imagen dada la ruta\n",
        "def classify_image(image_path: str):\n",
        "    \"\"\"\n",
        "    Clasifica una imagen usando VGG16 preentrenado en ImageNet.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): ruta al archivo de imagen (JPEG/PNG).\n",
        "\n",
        "    Returns:\n",
        "        str: nombre de la clase predicha.\n",
        "    \"\"\"\n",
        "    model, preprocess, categories = load_pretrained_vgg()\n",
        "    # Abrir imagen y convertir a RGB\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    # Aplicar transformaciones y crear batch (agregar dimensión batch)\n",
        "    input_tensor = preprocess(image).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_tensor)\n",
        "    predicted_idx = outputs.argmax(dim=1).item()\n",
        "    predicted_class = categories[predicted_idx]\n",
        "    return predicted_class"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = 'imagen.jpg'\n",
        "print('Predicción:', classify_image(image_path))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoRjQXv3qFcw",
        "outputId": "6e0d4096-827c-4066-fc06-1af60613a2cb"
      },
      "id": "QoRjQXv3qFcw",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicción: lorikeet\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}