{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RodrigoGuedesDP/Computer_Vision/blob/main/Sesion07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c84c763e",
      "metadata": {
        "id": "c84c763e"
      },
      "source": [
        "# Redes Neuronales Convolucionales"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcf4f199",
      "metadata": {
        "id": "bcf4f199"
      },
      "source": [
        "## Max Pooling (Submuestreo máximo)\n",
        "\n",
        "El **max pooling** es una operación de submuestreo utilizada habitualmente en redes neuronales convolucionales. Consiste en recorrer la imagen con una ventana de un tamaño determinado (p. ej. 2×2) y, para cada región, mantener únicamente el valor máximo. Esto reduce la resolución espacial de las características, hace la representación más compacta y proporciona invariancia a pequeñas translaciones.\n",
        "\n",
        "En el siguiente ejemplo trabajaremos con una única imagen de un canal (es decir, una matriz 4×4) y aplicaremos una capa de max pooling con ventana 2×2 y paso (`stride`) 2. La salida tendrá la mitad de la altura y la mitad del ancho de la entrada.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6ebcc96",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6ebcc96",
        "outputId": "236f8d60-6783-4a0d-d74c-a1691deb8be7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma de la entrada: torch.Size([1, 1, 4, 4])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Definimos una entrada de ejemplo: 1 imagen (batch), 1 canal, tamaño 4×4.\n",
        "# Cada número se incrementa para poder ver claramente el efecto del pooling.\n",
        "x = torch.tensor([[[[1.0, 2.0, 3.0, 4.0],\n",
        "                    [5.0, 6.0, 7.0, 8.0],\n",
        "                    [9.0,10.0,11.0,12.0],\n",
        "                    [13.0,14.0,15.0,16.0]]]])\n",
        "\n",
        "print(f\"Forma de la entrada: {x.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fe8e3c2",
      "metadata": {
        "id": "9fe8e3c2"
      },
      "outputs": [],
      "source": [
        "# Creamos la capa de max pooling con ventana 2×2 y stride 2.\n",
        "pool = nn.MaxPool2d(kernel_size=2, stride=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea6f97db",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea6f97db",
        "outputId": "6170eed2-98b0-43ac-d7de-ce1a3a280618"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultado del max pooling:\n",
            "tensor([[[[ 6.,  8.],\n",
            "          [14., 16.]]]])\n",
            "Forma de la salida: torch.Size([1, 1, 2, 2])\n"
          ]
        }
      ],
      "source": [
        "# Aplicamos el max pooling a la imagen de entrada\n",
        "y = pool(x)\n",
        "print(\"Resultado del max pooling:\")\n",
        "print(y)\n",
        "print(f\"Forma de la salida: {y.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17752729",
      "metadata": {
        "id": "17752729"
      },
      "source": [
        "## Convolución 2D\n",
        "\n",
        "Las **convoluciones** son el núcleo de las redes convolucionales. Se aplican filtros (o kernels) que se deslizan sobre la imagen para extraer patrones locales como bordes, texturas y formas. En PyTorch, una capa `nn.Conv2d` recibe un tensor de entrada de forma `(batch_size, canales, altura, ancho)` y aplica `out_channels` filtros de tamaño `kernel_size`.\n",
        "\n",
        "A continuación definiremos un pequeño ejemplo con una entrada 4×4 de un solo canal y un filtro 3×3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "912efb36",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "912efb36",
        "outputId": "72546d2c-457c-4e41-c281-5c3a2ea0da28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma de la entrada: torch.Size([1, 1, 4, 4])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Entrada: 1 imagen, 1 canal, tamaño 4×4\n",
        "x = torch.tensor([[[[1.0, 2.0, 3.0, 4.0],\n",
        "                    [5.0, 6.0, 7.0, 8.0],\n",
        "                    [9.0,10.0,11.0,12.0],\n",
        "                    [13.0,14.0,15.0,16.0]]]])\n",
        "print(f\"Forma de la entrada: {x.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bdc9cf3",
      "metadata": {
        "id": "8bdc9cf3"
      },
      "outputs": [],
      "source": [
        "# Creamos una capa convolucional con un filtro 3×3\n",
        "# in_channels=1 porque la entrada tiene 1 canal y out_channels=1 porque queremos un único mapa de salida.\n",
        "conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, bias=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4041b9b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4041b9b",
        "outputId": "b660cf23-f58f-4659-c809-3844b1dd2aa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mapa de activaciones resultante:\n",
            "tensor([[[[4.8553, 4.9304],\n",
            "          [5.1555, 5.2306]]]], grad_fn=<ConvolutionBackward0>)\n",
            "Forma de la salida: torch.Size([1, 1, 2, 2])\n"
          ]
        }
      ],
      "source": [
        "# Aplicamos la convolución\n",
        "y = conv(x)\n",
        "print(\"Mapa de activaciones resultante:\")\n",
        "print(y)\n",
        "print(f\"Forma de la salida: {y.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b2562ba",
      "metadata": {
        "id": "0b2562ba"
      },
      "source": [
        "## Capas convolucionales y mapas de activación\n",
        "\n",
        "Cuando encadenamos varias capas convolucionales, cada una produce un **mapa de activaciones** con una cierta altura y anchura.\n",
        "El tamaño de salida `(H_out, W_out)` de una convolución sin padding se calcula mediante:\n",
        "\n",
        "$$\n",
        "H_{\\text{out}} = \\left\\lfloor \\frac{H_{\\text{in}} - \\text{kernel_size}}{\\text{stride}} \\right\\rfloor + 1\n",
        "$$\n",
        "\n",
        "$$\n",
        "W_{\\text{out}} = \\left\\lfloor \\frac{W_{\\text{in}} - \\text{kernel_size}}{\\text{stride}} \\right\\rfloor + 1\n",
        "$$\n",
        "\n",
        "\n",
        "Esta fórmula te permite prever cómo disminuyen las dimensiones espaciales a medida que aplicas filtros. En el siguiente ejemplo crearemos un tensor de entrada aleatorio y veremos las dimensiones de salida tras aplicar dos capas convolucionales consecutivas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33fde193",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33fde193",
        "outputId": "6f1a0d82-b5ec-4c08-9f94-7cc35b959d4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma de la entrada: torch.Size([1, 3, 19, 19])\n",
            "Forma de los filtros de la primera capa: torch.Size([7, 3, 5, 5])\n",
            "Forma del mapa de activaciones tras conv1: torch.Size([1, 7, 15, 15])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Creamos una entrada aleatoria: lote de tamaño 1, 3 canales, 19×19 píxeles\n",
        "x = torch.randn(1, 3, 19, 19)\n",
        "\n",
        "# Primera capa convolucional: 3 canales de entrada → 7 canales de salida, kernel 5×5\n",
        "conv1 = nn.Conv2d(in_channels=3, out_channels=7, kernel_size=5, bias=False)\n",
        "feature_map1 = conv1(x)\n",
        "\n",
        "print(f\"Forma de la entrada: {x.shape}\")\n",
        "print(f\"Forma de los filtros de la primera capa: {conv1.weight.shape}\")\n",
        "print(f\"Forma del mapa de activaciones tras conv1: {feature_map1.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c86fd212",
      "metadata": {
        "id": "c86fd212"
      },
      "source": [
        "En este caso, la entrada tenía tamaño 19×19 y un `kernel_size` de 5 con `stride=1`. Por tanto, cada dimensión disminuye a **19 − 5 + 1 = 15**, lo que concuerda con la forma de salida `(1, 7, 15, 15)`.\n",
        "\n",
        "Si añadimos otra capa convolucional con `kernel_size=3` y `in_channels=7`, el tamaño volverá a disminuir en 2 píxeles por dimensión (15 − 3 + 1 = 13).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34a6f64c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34a6f64c",
        "outputId": "c8d69c11-1597-461c-d1da-f88e765475ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma de los filtros de la segunda capa: torch.Size([2, 7, 3, 3])\n",
            "Forma del mapa de activaciones tras conv2: torch.Size([1, 2, 13, 13])\n"
          ]
        }
      ],
      "source": [
        "# Segunda capa convolucional: toma los 7 canales de salida de conv1 y produce 2 canales de salida\n",
        "conv2 = nn.Conv2d(in_channels=7, out_channels=2, kernel_size=3, bias=False)\n",
        "feature_map2 = conv2(feature_map1)\n",
        "\n",
        "print(f\"Forma de los filtros de la segunda capa: {conv2.weight.shape}\")\n",
        "print(f\"Forma del mapa de activaciones tras conv2: {feature_map2.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "063dae52",
      "metadata": {
        "id": "063dae52"
      },
      "source": [
        "## Modelos preentrenados: AlexNet y VGG16\n",
        "\n",
        "Las bibliotecas de visión por computadora modernas incluyen modelos previamente entrenados sobre grandes conjuntos de datos como **ImageNet**. Estos modelos han aprendido representaciones útiles que pueden reutilizarse para tareas nuevas (lo que se conoce como _transfer learning_).\n",
        "\n",
        "Usaremos `torchvision.models` para cargar dos arquitecturas clásicas:\n",
        "\n",
        "- **AlexNet**: propuesto en 2012, con cinco capas convolucionales y tres capas completamente conectadas.\n",
        "- **VGG16**: introducido en 2014, con una arquitectura más profunda basada únicamente en filtros 3×3.\n",
        "\n",
        "También utilizaremos la función `summary` de `torchinfo` para resumir las dimensiones de entrada y salida de cada capa.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5866d322",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5866d322",
        "outputId": "eab12273-18e5-45fa-f246-fd4e965d0867"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 233M/233M [00:01<00:00, 156MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 528M/528M [00:07<00:00, 71.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "# Cargamos los modelos AlexNet y VGG16 preentrenados en ImageNet\n",
        "alex = models.alexnet(weights='IMAGENET1K_V1')\n",
        "vgg  = models.vgg16(weights='IMAGENET1K_V1')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cefacda5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cefacda5",
        "outputId": "2b363122-69ec-4144-80ca-9277e24d5db2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "AlexNet                                  [1, 1000]                 --\n",
              "├─Sequential: 1-1                        [1, 256, 6, 6]            --\n",
              "│    └─Conv2d: 2-1                       [1, 64, 55, 55]           23,296\n",
              "│    └─ReLU: 2-2                         [1, 64, 55, 55]           --\n",
              "│    └─MaxPool2d: 2-3                    [1, 64, 27, 27]           --\n",
              "│    └─Conv2d: 2-4                       [1, 192, 27, 27]          307,392\n",
              "│    └─ReLU: 2-5                         [1, 192, 27, 27]          --\n",
              "│    └─MaxPool2d: 2-6                    [1, 192, 13, 13]          --\n",
              "│    └─Conv2d: 2-7                       [1, 384, 13, 13]          663,936\n",
              "│    └─ReLU: 2-8                         [1, 384, 13, 13]          --\n",
              "│    └─Conv2d: 2-9                       [1, 256, 13, 13]          884,992\n",
              "│    └─ReLU: 2-10                        [1, 256, 13, 13]          --\n",
              "│    └─Conv2d: 2-11                      [1, 256, 13, 13]          590,080\n",
              "│    └─ReLU: 2-12                        [1, 256, 13, 13]          --\n",
              "│    └─MaxPool2d: 2-13                   [1, 256, 6, 6]            --\n",
              "├─AdaptiveAvgPool2d: 1-2                 [1, 256, 6, 6]            --\n",
              "├─Sequential: 1-3                        [1, 1000]                 --\n",
              "│    └─Dropout: 2-14                     [1, 9216]                 --\n",
              "│    └─Linear: 2-15                      [1, 4096]                 37,752,832\n",
              "│    └─ReLU: 2-16                        [1, 4096]                 --\n",
              "│    └─Dropout: 2-17                     [1, 4096]                 --\n",
              "│    └─Linear: 2-18                      [1, 4096]                 16,781,312\n",
              "│    └─ReLU: 2-19                        [1, 4096]                 --\n",
              "│    └─Linear: 2-20                      [1, 1000]                 4,097,000\n",
              "==========================================================================================\n",
              "Total params: 61,100,840\n",
              "Trainable params: 61,100,840\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 714.68\n",
              "==========================================================================================\n",
              "Input size (MB): 0.60\n",
              "Forward/backward pass size (MB): 3.95\n",
              "Params size (MB): 244.40\n",
              "Estimated Total Size (MB): 248.96\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Instalamos e importamos torchinfo para visualizar un resumen de los modelos\n",
        "!pip install -q torchinfo\n",
        "from torchinfo import summary\n",
        "\n",
        "# Mostramos el resumen de AlexNet para una entrada de una imagen (batch=1) de 3 canales y 224×224 píxeles\n",
        "summary(alex, input_size=(1, 3, 224, 224))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe7c960f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe7c960f",
        "outputId": "e0251302-d04d-4b33-c265-f2689670e947"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "VGG                                      [1, 1000]                 --\n",
              "├─Sequential: 1-1                        [1, 512, 7, 7]            --\n",
              "│    └─Conv2d: 2-1                       [1, 64, 224, 224]         1,792\n",
              "│    └─ReLU: 2-2                         [1, 64, 224, 224]         --\n",
              "│    └─Conv2d: 2-3                       [1, 64, 224, 224]         36,928\n",
              "│    └─ReLU: 2-4                         [1, 64, 224, 224]         --\n",
              "│    └─MaxPool2d: 2-5                    [1, 64, 112, 112]         --\n",
              "│    └─Conv2d: 2-6                       [1, 128, 112, 112]        73,856\n",
              "│    └─ReLU: 2-7                         [1, 128, 112, 112]        --\n",
              "│    └─Conv2d: 2-8                       [1, 128, 112, 112]        147,584\n",
              "│    └─ReLU: 2-9                         [1, 128, 112, 112]        --\n",
              "│    └─MaxPool2d: 2-10                   [1, 128, 56, 56]          --\n",
              "│    └─Conv2d: 2-11                      [1, 256, 56, 56]          295,168\n",
              "│    └─ReLU: 2-12                        [1, 256, 56, 56]          --\n",
              "│    └─Conv2d: 2-13                      [1, 256, 56, 56]          590,080\n",
              "│    └─ReLU: 2-14                        [1, 256, 56, 56]          --\n",
              "│    └─Conv2d: 2-15                      [1, 256, 56, 56]          590,080\n",
              "│    └─ReLU: 2-16                        [1, 256, 56, 56]          --\n",
              "│    └─MaxPool2d: 2-17                   [1, 256, 28, 28]          --\n",
              "│    └─Conv2d: 2-18                      [1, 512, 28, 28]          1,180,160\n",
              "│    └─ReLU: 2-19                        [1, 512, 28, 28]          --\n",
              "│    └─Conv2d: 2-20                      [1, 512, 28, 28]          2,359,808\n",
              "│    └─ReLU: 2-21                        [1, 512, 28, 28]          --\n",
              "│    └─Conv2d: 2-22                      [1, 512, 28, 28]          2,359,808\n",
              "│    └─ReLU: 2-23                        [1, 512, 28, 28]          --\n",
              "│    └─MaxPool2d: 2-24                   [1, 512, 14, 14]          --\n",
              "│    └─Conv2d: 2-25                      [1, 512, 14, 14]          2,359,808\n",
              "│    └─ReLU: 2-26                        [1, 512, 14, 14]          --\n",
              "│    └─Conv2d: 2-27                      [1, 512, 14, 14]          2,359,808\n",
              "│    └─ReLU: 2-28                        [1, 512, 14, 14]          --\n",
              "│    └─Conv2d: 2-29                      [1, 512, 14, 14]          2,359,808\n",
              "│    └─ReLU: 2-30                        [1, 512, 14, 14]          --\n",
              "│    └─MaxPool2d: 2-31                   [1, 512, 7, 7]            --\n",
              "├─AdaptiveAvgPool2d: 1-2                 [1, 512, 7, 7]            --\n",
              "├─Sequential: 1-3                        [1, 1000]                 --\n",
              "│    └─Linear: 2-32                      [1, 4096]                 102,764,544\n",
              "│    └─ReLU: 2-33                        [1, 4096]                 --\n",
              "│    └─Dropout: 2-34                     [1, 4096]                 --\n",
              "│    └─Linear: 2-35                      [1, 4096]                 16,781,312\n",
              "│    └─ReLU: 2-36                        [1, 4096]                 --\n",
              "│    └─Dropout: 2-37                     [1, 4096]                 --\n",
              "│    └─Linear: 2-38                      [1, 1000]                 4,097,000\n",
              "==========================================================================================\n",
              "Total params: 138,357,544\n",
              "Trainable params: 138,357,544\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 15.48\n",
              "==========================================================================================\n",
              "Input size (MB): 0.60\n",
              "Forward/backward pass size (MB): 108.45\n",
              "Params size (MB): 553.43\n",
              "Estimated Total Size (MB): 662.49\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Mostramos el resumen de VGG16 para la misma entrada\n",
        "summary(vgg, input_size=(1, 3, 224, 224))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "249dfced",
      "metadata": {
        "id": "249dfced"
      },
      "source": [
        "## Implementación de AlexNet desde cero\n",
        "\n",
        "Para comprender mejor el funcionamiento interno de AlexNet podemos implementar la red manualmente utilizando `torch.nn`. La versión original de 2012 constaba de:\n",
        "\n",
        "1. **Cinco capas convolucionales** con funciones de activación ReLU y capas de max pooling intercaladas.\n",
        "2. **Tres capas completamente conectadas**, con funciones de activación ReLU y dropout entre ellas.\n",
        "3. Un clasificador final que produce probabilidades para 1000 clases de ImageNet.\n",
        "\n",
        "A continuación se muestra una implementación fiel a la arquitectura original. Observa cómo se utiliza `nn.Sequential` para encadenar las capas y cómo se aplanan (`torch.flatten`) las salidas antes del clasificador.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df560a11",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df560a11",
        "outputId": "83d1efb6-2118-4abd-838f-867ea4032291"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma de la salida del modelo: torch.Size([1, 1000])\n",
            "Total de parámetros de AlexNet: 61.1 millones\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AlexNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementación de la arquitectura AlexNet de 2012.\n",
        "\n",
        "    Args:\n",
        "        num_classes (int): número de clases de salida. Por defecto 1000 (ImageNet).\n",
        "        in_channels (int): número de canales de entrada de la imagen. Por defecto 3.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes: int = 1000, in_channels: int = 3):\n",
        "        super().__init__()\n",
        "\n",
        "        # Bloques convolucionales: cada bloque está formado por una convolución,\n",
        "        # una activación ReLU y opcionalmente un max pooling.\n",
        "        self.features = nn.Sequential(\n",
        "            # Bloque 1: kernel 11×11, stride 4, padding 2\n",
        "            nn.Conv2d(in_channels, 64, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "\n",
        "            # Bloque 2: kernel 5×5\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "\n",
        "            # Bloque 3\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # Bloque 4\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # Bloque 5\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        )\n",
        "\n",
        "        # Clasificador: aplanamos las características y pasamos por capas densas con dropout\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(256 * 6 * 6, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(4096, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Propagación hacia adelante: obtenemos las características y las aplanamos\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)  # aplanar excepto la dimensión del batch\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Instanciamos el modelo para 1000 clases\n",
        "model = AlexNet(num_classes=1000)\n",
        "\n",
        "# Creamos una entrada dummy: lote de 1 imagen RGB de 224×224\n",
        "input_tensor = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "# Calculamos la salida del modelo\n",
        "y = model(input_tensor)\n",
        "print(f\"Forma de la salida del modelo: {y.shape}\")  # Debería ser (1, 1000)\n",
        "\n",
        "# Calculamos y mostramos el número total de parámetros (en millones)\n",
        "params = sum(p.numel() for p in model.parameters()) / 1e6\n",
        "print(f\"Total de parámetros de AlexNet: {params:.1f} millones\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36294a40",
      "metadata": {
        "id": "36294a40"
      },
      "source": [
        "En la implementación anterior observamos que la salida del modelo tiene forma `(1, 1000)`,\n",
        "correspondiente al número de clases de ImageNet. Además, el modelo contiene alrededor de **61,1 M** de parámetros aprendibles.\n",
        "\n",
        "Este recuento de parámetros se obtiene sumando todos los pesos y sesgos de las capas convolucionales y totalmente conectadas.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model, input_size=(1, 3, 224, 224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vf2anvnOpTin",
        "outputId": "b2beee3a-4acf-453d-e4cf-91c9c7ae0414"
      },
      "id": "vf2anvnOpTin",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "AlexNet                                  [1, 1000]                 --\n",
              "├─Sequential: 1-1                        [1, 256, 6, 6]            --\n",
              "│    └─Conv2d: 2-1                       [1, 64, 55, 55]           23,296\n",
              "│    └─ReLU: 2-2                         [1, 64, 55, 55]           --\n",
              "│    └─MaxPool2d: 2-3                    [1, 64, 27, 27]           --\n",
              "│    └─Conv2d: 2-4                       [1, 192, 27, 27]          307,392\n",
              "│    └─ReLU: 2-5                         [1, 192, 27, 27]          --\n",
              "│    └─MaxPool2d: 2-6                    [1, 192, 13, 13]          --\n",
              "│    └─Conv2d: 2-7                       [1, 384, 13, 13]          663,936\n",
              "│    └─ReLU: 2-8                         [1, 384, 13, 13]          --\n",
              "│    └─Conv2d: 2-9                       [1, 256, 13, 13]          884,992\n",
              "│    └─ReLU: 2-10                        [1, 256, 13, 13]          --\n",
              "│    └─Conv2d: 2-11                      [1, 256, 13, 13]          590,080\n",
              "│    └─ReLU: 2-12                        [1, 256, 13, 13]          --\n",
              "│    └─MaxPool2d: 2-13                   [1, 256, 6, 6]            --\n",
              "├─Sequential: 1-2                        [1, 1000]                 --\n",
              "│    └─Dropout: 2-14                     [1, 9216]                 --\n",
              "│    └─Linear: 2-15                      [1, 4096]                 37,752,832\n",
              "│    └─ReLU: 2-16                        [1, 4096]                 --\n",
              "│    └─Dropout: 2-17                     [1, 4096]                 --\n",
              "│    └─Linear: 2-18                      [1, 4096]                 16,781,312\n",
              "│    └─ReLU: 2-19                        [1, 4096]                 --\n",
              "│    └─Linear: 2-20                      [1, 1000]                 4,097,000\n",
              "==========================================================================================\n",
              "Total params: 61,100,840\n",
              "Trainable params: 61,100,840\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 714.68\n",
              "==========================================================================================\n",
              "Input size (MB): 0.60\n",
              "Forward/backward pass size (MB): 3.95\n",
              "Params size (MB): 244.40\n",
              "Estimated Total Size (MB): 248.96\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}